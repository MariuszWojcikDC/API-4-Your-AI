# ğŸ Python OpenAIâ€‘Compatible APIs - Deep Dive

## ğŸ“Œ Read This First
- This folder represents **one application expressed at multiple complexity tiers** - pick the tier that matches your skills and integration needs.  
- All variants talk to the same **Bielik 4.5B Ollama model** (`MODEL_NAME` is identical), returning **OpenAIâ€‘shaped responses** so you can switch tiers without rewriting clients.  
- Treat each script as a **learning template**: clone, modify, rerun, and observe how payloads, token usage, and SQL Server storage behave.

---

# ğŸ§— Complexity Ladder - What Each Tier Gives You

### **1ï¸âƒ£ `demo_minimal_embeddings.py` - Minimal FastAPI**
- Tiny surface, two endpoints, **no token accounting**.  
- Ideal for smokeâ€‘testing **Ollama connectivity** or teaching request/response structures.

### **2ï¸âƒ£ `embeddings_api_basic.py` - Add OpenAIâ€‘style `index` + `usage`**
- Good for **real clients** that track token usage.  
- Adds OpenAIâ€‘shaped response fields.

### **3ï¸âƒ£ `openai_compatible_api.py` - Full Dropâ€‘In OpenAI API**
- Implements **embeddings**, **completions**, and **chat completions** with `choices`, `usage`, and message roles.  
- Suitable for SDKs/tools expecting OpenAI schema.

### **4ï¸âƒ£ `ollama_api_project_with_pca` - Modular + SQL Serverâ€“Ready**
- Package layout with routers/services.  
- Includes **PCA reduction** to fit SQL Server 2025â€™s **VECTOR(1998)** limit while Bielik outputs **2,048 dimensions**.

---

# ğŸ” Detailed Walkthroughs (Codeâ€‘Line References)

## â­ `demo_minimal_embeddings.py`
- Model is hardâ€‘coded at **line 8**.  
- Simple request/response models (`10â€“18`).  
- `/generate` proxies directly to `ollama.chat` (`21â€“27`).  
- `/openai/deployments/local/embeddings` handles list/string inputs, loops, and calls `ollama.embeddings` (`31â€“42`).  
- Returns OpenAIâ€‘shaped `data[]` without usage totals.

---

## â­ `embeddings_api_basic.py`
- Same model anchor (`line 8`).  
- Adds `index` + `usage` schemas (`14â€“25`).  
- Normalizes input â†’ **one `ollama.embed` call** per request (`28â€“43`).  
- Token usage is computed from Ollama counters (`46â€“55`).

---

## â­ `openai_compatible_api.py`
- Full schema block (`10â€“58`): embeddings, completions, chat completions.  
- Embeddings: always uses Bielik (`61â€“88`).  
- Completions: wraps Ollama output in OpenAI `choices[0].text` (`90â€“125`).  
- Chat completions: converts OpenAI â†’ Ollama schema â†’ back (`133â€“168`).  
- Same `MODEL_NAME` everywhere (`line 8`).

---

# ğŸ§® PCAâ€‘Enabled API for SQL Server 2025

### Why PCA?
SQL Server 2025 limits vectors to **1,998 dims**.  
Bielik emits **2,048 dims** â†’ direct inserts fail.

### PCA Workflow
- Constants + PCA file path: `dim_reduction.py:6â€“9`.  
- Train with sample vectors: `train_pca()` (`18â€“26`).  
- Runtime reduction: `reduce_embedding()` (`28â€“40`).  
- If PCA missing â†’ safe fallback: **slice first 1,998 dims**.

### Package Layout
- Entrypoint wires routers: `main.py:1â€“8`.  
- Model config: `config.py:1`.  
- Schemas: `openai_schemas.py:5â€“49`.  
- Ollama service wrapper: `ollama_service.py:4â€“10`.  
- Embeddings router: `embeddings.py:8â€“28`.  
- Completions router: `completions.py:7â€“21`.  
- Chat router: `chat_completions.py:7â€“24`.

### Run Command
From `python/ollama_api_project_with_pca`:
```
uvicorn app.main:app --reload
```
TLS example (from `run_app_command.txt:1`):
```
uvicorn app.main:app --host 127.0.0.1 --port 5001 --ssl-certfile "...cert.crt" --ssl-keyfile "...cert.key"
```

---

# ğŸ› ï¸ Adaptation & Selfâ€‘Learning Checklist

- **Pick your tier:** start minimal â†’ basic â†’ compatible â†’ PCA.  
- **Swap models safely:** always verify embedding dimension via `ollama.embed`.  
- **Train PCA:** gather several hundred sample embeddings â†’ run `train_pca()`.  
- **Ensure SQL compatibility:** VECTOR column must match PCA output size.  
- **Understand fallback:** slicing works but degrades quality â†’ train PCA ASAP.  
- **Extend endpoints:** reuse schemas from `openai_compatible_api.py` or PCA package.  
- **Iterative testing:** validate token usage before scaling workloads.

---

# ğŸ¯ Key Takeaway
This repo is a **layered, OpenAIâ€‘compatible template** for experimentation or production.  
The main operational constraint:  
**SQL Server 2025 VECTOR(1998) vs. Bielikâ€™s 2,048â€‘dim embeddings â†’ PCA is mandatory.**

Whenever you change models, quantization, or providers, **verify embedding size first**, then choose the tier that fits your goals.

